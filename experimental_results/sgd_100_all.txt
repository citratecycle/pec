I'm very vegetable. I know nothing about what I'm doing right now, and currently I only want to touch fish and hua water.


optimizer: sgd
batch size: 128
epoch number: 100
learning rate: 0.0005
training mode: all


the parameter names and shapes for model is:
name: conv1.weight, parameter shape: torch.Size([16, 3, 3, 3])
name: conv1.bias, parameter shape: torch.Size([16])
name: conv2.weight, parameter shape: torch.Size([32, 16, 3, 3])
name: conv2.bias, parameter shape: torch.Size([32])
name: conv3.weight, parameter shape: torch.Size([64, 32, 3, 3])
name: conv3.bias, parameter shape: torch.Size([64])
name: conv4.weight, parameter shape: torch.Size([128, 64, 3, 3])
name: conv4.bias, parameter shape: torch.Size([128])
name: fc1.weight, parameter shape: torch.Size([1024, 3200])
name: fc1.bias, parameter shape: torch.Size([1024])
name: fc2.weight, parameter shape: torch.Size([10, 1024])
name: fc2.bias, parameter shape: torch.Size([10])
name: exit_1.a, parameter shape: torch.Size([1])
name: exit_1.c, parameter shape: torch.Size([1])
name: exit_1.n1, parameter shape: torch.Size([1])
name: exit_1.n2, parameter shape: torch.Size([1])
name: exit_1.codebook.weight, parameter shape: torch.Size([64, 32, 1, 1])
name: exit_1.codebook.bias, parameter shape: torch.Size([64])
name: exit_1_fc.weight, parameter shape: torch.Size([10, 64])
name: exit_1_fc.bias, parameter shape: torch.Size([10])
name: exit_2.a, parameter shape: torch.Size([1])
name: exit_2.c, parameter shape: torch.Size([1])
name: exit_2.n1, parameter shape: torch.Size([1])
name: exit_2.n2, parameter shape: torch.Size([1])
name: exit_2.codebook.weight, parameter shape: torch.Size([64, 128, 1, 1])
name: exit_2.codebook.bias, parameter shape: torch.Size([64])
Files already downloaded and verified
cuda

Epoch: 101
[101,   100] loss: 0.11514 |  Acc: 9.992% (1279/12800)
[101,   200] loss: 0.11513 |  Acc: 10.234% (1310/12800)
[101,   300] loss: 0.11513 |  Acc: 9.922% (1270/12800)
Accuracy of the network on the 10000 test images: 10 %
Accuracy of plane :  0 %
Accuracy of   car :  0 %
Accuracy of  bird :  0 %
Accuracy of   cat :  0 %
Accuracy of  deer : 100 %
Accuracy of   dog :  0 %
Accuracy of  frog :  0 %
Accuracy of horse :  0 %
Accuracy of  ship :  0 %
Accuracy of truck :  0 %
louchenfei@thinklab-61000:biyesheji$ /home/liuchang/anaconda3/bin/python main.py 
I'm very vegetable. I know nothing about what I'm doing right now, and currently I only want to touch fish and hua water.
the parameter names and shapes for model is:
name: conv1.weight, parameter shape: torch.Size([16, 3, 3, 3])
name: conv1.bias, parameter shape: torch.Size([16])
name: conv2.weight, parameter shape: torch.Size([32, 16, 3, 3])
name: conv2.bias, parameter shape: torch.Size([32])
name: conv3.weight, parameter shape: torch.Size([64, 32, 3, 3])
name: conv3.bias, parameter shape: torch.Size([64])
name: conv4.weight, parameter shape: torch.Size([128, 64, 3, 3])
name: conv4.bias, parameter shape: torch.Size([128])
name: fc1.weight, parameter shape: torch.Size([1024, 3200])
name: fc1.bias, parameter shape: torch.Size([1024])
name: fc2.weight, parameter shape: torch.Size([10, 1024])
name: fc2.bias, parameter shape: torch.Size([10])
name: exit_1.a, parameter shape: torch.Size([1])
name: exit_1.c, parameter shape: torch.Size([1])
name: exit_1.n1, parameter shape: torch.Size([1])
name: exit_1.n2, parameter shape: torch.Size([1])
name: exit_1.codebook.weight, parameter shape: torch.Size([64, 32, 1, 1])
name: exit_1.codebook.bias, parameter shape: torch.Size([64])
name: exit_1_fc.weight, parameter shape: torch.Size([10, 64])
name: exit_1_fc.bias, parameter shape: torch.Size([10])
name: exit_2.a, parameter shape: torch.Size([1])
name: exit_2.c, parameter shape: torch.Size([1])
name: exit_2.n1, parameter shape: torch.Size([1])
name: exit_2.n2, parameter shape: torch.Size([1])
name: exit_2.codebook.weight, parameter shape: torch.Size([64, 128, 1, 1])
name: exit_2.codebook.bias, parameter shape: torch.Size([64])
Files already downloaded and verified
cuda

Epoch: 1
[1,   100] loss: 0.11514 |  Acc: 9.953% (1274/12800)
[1,   200] loss: 0.11514 |  Acc: 10.219% (1308/12800)
[1,   300] loss: 0.11514 |  Acc: 10.375% (1328/12800)

Epoch: 2
[2,   100] loss: 0.11514 |  Acc: 10.344% (1324/12800)
[2,   200] loss: 0.11515 |  Acc: 9.766% (1250/12800)
[2,   300] loss: 0.11513 |  Acc: 10.461% (1339/12800)

Epoch: 3
[3,   100] loss: 0.11512 |  Acc: 10.547% (1350/12800)
[3,   200] loss: 0.11513 |  Acc: 10.508% (1345/12800)
[3,   300] loss: 0.11512 |  Acc: 10.875% (1392/12800)

Epoch: 4
[4,   100] loss: 0.11512 |  Acc: 10.977% (1405/12800)
[4,   200] loss: 0.11513 |  Acc: 10.523% (1347/12800)
[4,   300] loss: 0.11511 |  Acc: 11.117% (1423/12800)

Epoch: 5
[5,   100] loss: 0.11512 |  Acc: 10.352% (1325/12800)
[5,   200] loss: 0.11511 |  Acc: 10.562% (1352/12800)
[5,   300] loss: 0.11511 |  Acc: 10.602% (1357/12800)

Epoch: 6
[6,   100] loss: 0.11510 |  Acc: 11.211% (1435/12800)
[6,   200] loss: 0.11509 |  Acc: 11.000% (1408/12800)
[6,   300] loss: 0.11510 |  Acc: 11.023% (1411/12800)

Epoch: 7
[7,   100] loss: 0.11510 |  Acc: 10.539% (1349/12800)
[7,   200] loss: 0.11510 |  Acc: 11.609% (1486/12800)
[7,   300] loss: 0.11509 |  Acc: 11.445% (1465/12800)

Epoch: 8
[8,   100] loss: 0.11509 |  Acc: 11.164% (1429/12800)
[8,   200] loss: 0.11509 |  Acc: 11.172% (1430/12800)
[8,   300] loss: 0.11509 |  Acc: 10.875% (1392/12800)

Epoch: 9
[9,   100] loss: 0.11509 |  Acc: 11.492% (1471/12800)
[9,   200] loss: 0.11509 |  Acc: 11.047% (1414/12800)
[9,   300] loss: 0.11509 |  Acc: 10.844% (1388/12800)

Epoch: 10
[10,   100] loss: 0.11508 |  Acc: 11.398% (1459/12800)
[10,   200] loss: 0.11508 |  Acc: 10.992% (1407/12800)
[10,   300] loss: 0.11508 |  Acc: 10.977% (1405/12800)

Epoch: 11
[11,   100] loss: 0.11508 |  Acc: 10.812% (1384/12800)
[11,   200] loss: 0.11507 |  Acc: 11.234% (1438/12800)
[11,   300] loss: 0.11507 |  Acc: 11.422% (1462/12800)

Epoch: 12
[12,   100] loss: 0.11507 |  Acc: 11.477% (1469/12800)
[12,   200] loss: 0.11506 |  Acc: 11.508% (1473/12800)
[12,   300] loss: 0.11506 |  Acc: 11.734% (1502/12800)

Epoch: 13
[13,   100] loss: 0.11506 |  Acc: 11.117% (1423/12800)
[13,   200] loss: 0.11505 |  Acc: 11.508% (1473/12800)
[13,   300] loss: 0.11506 |  Acc: 11.352% (1453/12800)

Epoch: 14
[14,   100] loss: 0.11505 |  Acc: 11.078% (1418/12800)
[14,   200] loss: 0.11505 |  Acc: 11.875% (1520/12800)
[14,   300] loss: 0.11504 |  Acc: 11.719% (1500/12800)

Epoch: 15
[15,   100] loss: 0.11505 |  Acc: 11.008% (1409/12800)
[15,   200] loss: 0.11505 |  Acc: 11.195% (1433/12800)
[15,   300] loss: 0.11505 |  Acc: 11.656% (1492/12800)

Epoch: 16
[16,   100] loss: 0.11503 |  Acc: 11.633% (1489/12800)
[16,   200] loss: 0.11503 |  Acc: 11.453% (1466/12800)
[16,   300] loss: 0.11503 |  Acc: 11.789% (1509/12800)

Epoch: 17
[17,   100] loss: 0.11503 |  Acc: 11.422% (1462/12800)
[17,   200] loss: 0.11502 |  Acc: 11.820% (1513/12800)
[17,   300] loss: 0.11502 |  Acc: 11.273% (1443/12800)

Epoch: 18
[18,   100] loss: 0.11501 |  Acc: 12.031% (1540/12800)
[18,   200] loss: 0.11500 |  Acc: 11.914% (1525/12800)
[18,   300] loss: 0.11502 |  Acc: 11.547% (1478/12800)

Epoch: 19
[19,   100] loss: 0.11500 |  Acc: 11.555% (1479/12800)
[19,   200] loss: 0.11500 |  Acc: 12.078% (1546/12800)
[19,   300] loss: 0.11502 |  Acc: 11.648% (1491/12800)

Epoch: 20
[20,   100] loss: 0.11500 |  Acc: 12.102% (1549/12800)
[20,   200] loss: 0.11499 |  Acc: 11.672% (1494/12800)
[20,   300] loss: 0.11499 |  Acc: 12.273% (1571/12800)

Epoch: 21
[21,   100] loss: 0.11501 |  Acc: 11.625% (1488/12800)
[21,   200] loss: 0.11498 |  Acc: 12.383% (1585/12800)
[21,   300] loss: 0.11497 |  Acc: 11.844% (1516/12800)

Epoch: 22
[22,   100] loss: 0.11497 |  Acc: 12.352% (1581/12800)
[22,   200] loss: 0.11495 |  Acc: 12.266% (1570/12800)
[22,   300] loss: 0.11497 |  Acc: 12.297% (1574/12800)

Epoch: 23
[23,   100] loss: 0.11496 |  Acc: 12.367% (1583/12800)
[23,   200] loss: 0.11495 |  Acc: 12.359% (1582/12800)
[23,   300] loss: 0.11495 |  Acc: 12.086% (1547/12800)

Epoch: 24
[24,   100] loss: 0.11496 |  Acc: 12.578% (1610/12800)
[24,   200] loss: 0.11493 |  Acc: 12.367% (1583/12800)
[24,   300] loss: 0.11495 |  Acc: 12.359% (1582/12800)

Epoch: 25
[25,   100] loss: 0.11493 |  Acc: 11.969% (1532/12800)
[25,   200] loss: 0.11493 |  Acc: 12.281% (1572/12800)
[25,   300] loss: 0.11492 |  Acc: 12.148% (1555/12800)

Epoch: 26
[26,   100] loss: 0.11491 |  Acc: 12.867% (1647/12800)
[26,   200] loss: 0.11492 |  Acc: 12.453% (1594/12800)
[26,   300] loss: 0.11490 |  Acc: 12.766% (1634/12800)

Epoch: 27
[27,   100] loss: 0.11489 |  Acc: 12.625% (1616/12800)
[27,   200] loss: 0.11489 |  Acc: 12.648% (1619/12800)
[27,   300] loss: 0.11488 |  Acc: 12.898% (1651/12800)

Epoch: 28
[28,   100] loss: 0.11487 |  Acc: 12.789% (1637/12800)
[28,   200] loss: 0.11487 |  Acc: 13.109% (1678/12800)
[28,   300] loss: 0.11487 |  Acc: 13.250% (1696/12800)

Epoch: 29
[29,   100] loss: 0.11484 |  Acc: 13.281% (1700/12800)
[29,   200] loss: 0.11484 |  Acc: 12.953% (1658/12800)
[29,   300] loss: 0.11483 |  Acc: 13.773% (1763/12800)

Epoch: 30
[30,   100] loss: 0.11486 |  Acc: 12.914% (1653/12800)
[30,   200] loss: 0.11480 |  Acc: 13.344% (1708/12800)
[30,   300] loss: 0.11481 |  Acc: 13.586% (1739/12800)

Epoch: 31
[31,   100] loss: 0.11480 |  Acc: 13.164% (1685/12800)
[31,   200] loss: 0.11480 |  Acc: 13.383% (1713/12800)
[31,   300] loss: 0.11475 |  Acc: 14.242% (1823/12800)

Epoch: 32
[32,   100] loss: 0.11477 |  Acc: 13.195% (1689/12800)
[32,   200] loss: 0.11476 |  Acc: 13.359% (1710/12800)
[32,   300] loss: 0.11475 |  Acc: 13.500% (1728/12800)

Epoch: 33
[33,   100] loss: 0.11474 |  Acc: 13.305% (1703/12800)
[33,   200] loss: 0.11472 |  Acc: 13.719% (1756/12800)
[33,   300] loss: 0.11473 |  Acc: 13.594% (1740/12800)

Epoch: 34
[34,   100] loss: 0.11470 |  Acc: 13.922% (1782/12800)
[34,   200] loss: 0.11469 |  Acc: 13.680% (1751/12800)
[34,   300] loss: 0.11467 |  Acc: 13.578% (1738/12800)

Epoch: 35
[35,   100] loss: 0.11465 |  Acc: 14.133% (1809/12800)
[35,   200] loss: 0.11459 |  Acc: 14.469% (1852/12800)
[35,   300] loss: 0.11462 |  Acc: 14.203% (1818/12800)

Epoch: 36
[36,   100] loss: 0.11459 |  Acc: 14.883% (1905/12800)
[36,   200] loss: 0.11455 |  Acc: 14.250% (1824/12800)
[36,   300] loss: 0.11456 |  Acc: 14.250% (1824/12800)

Epoch: 37
[37,   100] loss: 0.11451 |  Acc: 14.555% (1863/12800)
[37,   200] loss: 0.11451 |  Acc: 14.930% (1911/12800)
[37,   300] loss: 0.11447 |  Acc: 14.906% (1908/12800)

Epoch: 38
[38,   100] loss: 0.11441 |  Acc: 15.336% (1963/12800)
[38,   200] loss: 0.11441 |  Acc: 14.766% (1890/12800)
[38,   300] loss: 0.11443 |  Acc: 14.633% (1873/12800)

Epoch: 39
[39,   100] loss: 0.11433 |  Acc: 15.492% (1983/12800)
[39,   200] loss: 0.11434 |  Acc: 15.648% (2003/12800)
[39,   300] loss: 0.11433 |  Acc: 15.062% (1928/12800)

Epoch: 40
[40,   100] loss: 0.11423 |  Acc: 15.422% (1974/12800)
[40,   200] loss: 0.11425 |  Acc: 15.102% (1933/12800)
[40,   300] loss: 0.11423 |  Acc: 15.367% (1967/12800)

Epoch: 41
[41,   100] loss: 0.11415 |  Acc: 15.859% (2030/12800)
[41,   200] loss: 0.11412 |  Acc: 15.594% (1996/12800)
[41,   300] loss: 0.11408 |  Acc: 15.758% (2017/12800)

Epoch: 42
[42,   100] loss: 0.11404 |  Acc: 15.898% (2035/12800)
[42,   200] loss: 0.11396 |  Acc: 15.875% (2032/12800)
[42,   300] loss: 0.11394 |  Acc: 15.867% (2031/12800)

Epoch: 43
[43,   100] loss: 0.11390 |  Acc: 16.258% (2081/12800)
[43,   200] loss: 0.11381 |  Acc: 16.039% (2053/12800)
[43,   300] loss: 0.11369 |  Acc: 16.758% (2145/12800)

Epoch: 44
[44,   100] loss: 0.11365 |  Acc: 16.633% (2129/12800)
[44,   200] loss: 0.11354 |  Acc: 16.977% (2173/12800)
[44,   300] loss: 0.11357 |  Acc: 16.805% (2151/12800)

Epoch: 45
[45,   100] loss: 0.11333 |  Acc: 18.086% (2315/12800)
[45,   200] loss: 0.11339 |  Acc: 16.945% (2169/12800)
[45,   300] loss: 0.11332 |  Acc: 17.203% (2202/12800)

Epoch: 46
[46,   100] loss: 0.11312 |  Acc: 17.523% (2243/12800)
[46,   200] loss: 0.11300 |  Acc: 17.414% (2229/12800)
[46,   300] loss: 0.11305 |  Acc: 17.711% (2267/12800)

Epoch: 47
[47,   100] loss: 0.11283 |  Acc: 18.094% (2316/12800)
[47,   200] loss: 0.11262 |  Acc: 18.492% (2367/12800)
[47,   300] loss: 0.11263 |  Acc: 18.172% (2326/12800)

Epoch: 48
[48,   100] loss: 0.11245 |  Acc: 18.258% (2337/12800)
[48,   200] loss: 0.11220 |  Acc: 18.148% (2323/12800)
[48,   300] loss: 0.11212 |  Acc: 19.023% (2435/12800)

Epoch: 49
[49,   100] loss: 0.11189 |  Acc: 18.930% (2423/12800)
[49,   200] loss: 0.11171 |  Acc: 18.852% (2413/12800)
[49,   300] loss: 0.11152 |  Acc: 19.531% (2500/12800)

Epoch: 50
[50,   100] loss: 0.11110 |  Acc: 19.570% (2505/12800)
[50,   200] loss: 0.11105 |  Acc: 19.930% (2551/12800)
[50,   300] loss: 0.11081 |  Acc: 19.688% (2520/12800)

Epoch: 51
[51,   100] loss: 0.11041 |  Acc: 19.531% (2500/12800)
[51,   200] loss: 0.11018 |  Acc: 20.352% (2605/12800)
[51,   300] loss: 0.10980 |  Acc: 20.398% (2611/12800)

Epoch: 52
[52,   100] loss: 0.10945 |  Acc: 20.453% (2618/12800)
[52,   200] loss: 0.10930 |  Acc: 20.977% (2685/12800)
[52,   300] loss: 0.10891 |  Acc: 20.883% (2673/12800)

Epoch: 53
[53,   100] loss: 0.10848 |  Acc: 21.062% (2696/12800)
[53,   200] loss: 0.10805 |  Acc: 21.445% (2745/12800)
[53,   300] loss: 0.10798 |  Acc: 21.344% (2732/12800)

Epoch: 54
[54,   100] loss: 0.10738 |  Acc: 22.211% (2843/12800)
[54,   200] loss: 0.10718 |  Acc: 22.508% (2881/12800)
[54,   300] loss: 0.10712 |  Acc: 22.641% (2898/12800)

Epoch: 55
[55,   100] loss: 0.10702 |  Acc: 23.078% (2954/12800)
[55,   200] loss: 0.10667 |  Acc: 22.773% (2915/12800)
[55,   300] loss: 0.10648 |  Acc: 23.102% (2957/12800)

Epoch: 56
[56,   100] loss: 0.10628 |  Acc: 23.344% (2988/12800)
[56,   200] loss: 0.10616 |  Acc: 23.953% (3066/12800)
[56,   300] loss: 0.10586 |  Acc: 24.773% (3171/12800)

Epoch: 57
[57,   100] loss: 0.10611 |  Acc: 24.461% (3131/12800)
[57,   200] loss: 0.10553 |  Acc: 24.578% (3146/12800)
[57,   300] loss: 0.10588 |  Acc: 24.195% (3097/12800)

Epoch: 58
[58,   100] loss: 0.10557 |  Acc: 24.812% (3176/12800)
[58,   200] loss: 0.10545 |  Acc: 25.133% (3217/12800)
[58,   300] loss: 0.10541 |  Acc: 24.797% (3174/12800)

Epoch: 59
[59,   100] loss: 0.10527 |  Acc: 25.312% (3240/12800)
[59,   200] loss: 0.10490 |  Acc: 25.258% (3233/12800)
[59,   300] loss: 0.10486 |  Acc: 25.188% (3224/12800)

Epoch: 60
[60,   100] loss: 0.10482 |  Acc: 25.656% (3284/12800)
[60,   200] loss: 0.10513 |  Acc: 25.352% (3245/12800)
[60,   300] loss: 0.10467 |  Acc: 25.648% (3283/12800)

Epoch: 61
[61,   100] loss: 0.10513 |  Acc: 25.516% (3266/12800)
[61,   200] loss: 0.10395 |  Acc: 26.398% (3379/12800)
[61,   300] loss: 0.10466 |  Acc: 26.094% (3340/12800)

Epoch: 62
[62,   100] loss: 0.10478 |  Acc: 25.664% (3285/12800)
[62,   200] loss: 0.10364 |  Acc: 26.961% (3451/12800)
[62,   300] loss: 0.10427 |  Acc: 26.016% (3330/12800)

Epoch: 63
[63,   100] loss: 0.10410 |  Acc: 26.266% (3362/12800)
[63,   200] loss: 0.10385 |  Acc: 26.234% (3358/12800)
[63,   300] loss: 0.10447 |  Acc: 26.383% (3377/12800)

Epoch: 64
[64,   100] loss: 0.10362 |  Acc: 27.008% (3457/12800)
[64,   200] loss: 0.10383 |  Acc: 26.617% (3407/12800)
[64,   300] loss: 0.10391 |  Acc: 26.461% (3387/12800)

Epoch: 65
[65,   100] loss: 0.10374 |  Acc: 26.961% (3451/12800)
[65,   200] loss: 0.10331 |  Acc: 27.008% (3457/12800)
[65,   300] loss: 0.10333 |  Acc: 26.953% (3450/12800)

Epoch: 66
[66,   100] loss: 0.10325 |  Acc: 26.992% (3455/12800)
[66,   200] loss: 0.10313 |  Acc: 27.445% (3513/12800)
[66,   300] loss: 0.10342 |  Acc: 27.172% (3478/12800)

Epoch: 67
[67,   100] loss: 0.10306 |  Acc: 27.305% (3495/12800)
[67,   200] loss: 0.10258 |  Acc: 27.281% (3492/12800)
[67,   300] loss: 0.10312 |  Acc: 27.172% (3478/12800)

Epoch: 68
[68,   100] loss: 0.10266 |  Acc: 27.867% (3567/12800)
[68,   200] loss: 0.10308 |  Acc: 27.461% (3515/12800)
[68,   300] loss: 0.10236 |  Acc: 28.273% (3619/12800)

Epoch: 69
[69,   100] loss: 0.10245 |  Acc: 27.773% (3555/12800)
[69,   200] loss: 0.10246 |  Acc: 27.508% (3521/12800)
[69,   300] loss: 0.10216 |  Acc: 28.289% (3621/12800)

Epoch: 70
[70,   100] loss: 0.10273 |  Acc: 27.797% (3558/12800)
[70,   200] loss: 0.10230 |  Acc: 27.836% (3563/12800)
[70,   300] loss: 0.10176 |  Acc: 28.203% (3610/12800)

Epoch: 71
[71,   100] loss: 0.10208 |  Acc: 28.156% (3604/12800)
[71,   200] loss: 0.10162 |  Acc: 28.570% (3657/12800)
[71,   300] loss: 0.10227 |  Acc: 27.742% (3551/12800)

Epoch: 72
[72,   100] loss: 0.10189 |  Acc: 28.336% (3627/12800)
[72,   200] loss: 0.10122 |  Acc: 29.156% (3732/12800)
[72,   300] loss: 0.10153 |  Acc: 29.352% (3757/12800)

Epoch: 73
[73,   100] loss: 0.10127 |  Acc: 28.945% (3705/12800)
[73,   200] loss: 0.10136 |  Acc: 28.703% (3674/12800)
[73,   300] loss: 0.10095 |  Acc: 29.219% (3740/12800)

Epoch: 74
[74,   100] loss: 0.10068 |  Acc: 29.008% (3713/12800)
[74,   200] loss: 0.10139 |  Acc: 28.633% (3665/12800)
[74,   300] loss: 0.10105 |  Acc: 29.305% (3751/12800)

Epoch: 75
[75,   100] loss: 0.10046 |  Acc: 29.531% (3780/12800)
[75,   200] loss: 0.10108 |  Acc: 29.250% (3744/12800)
[75,   300] loss: 0.10074 |  Acc: 28.891% (3698/12800)

Epoch: 76
[76,   100] loss: 0.10049 |  Acc: 29.555% (3783/12800)
[76,   200] loss: 0.10064 |  Acc: 29.227% (3741/12800)
[76,   300] loss: 0.10023 |  Acc: 29.867% (3823/12800)

Epoch: 77
[77,   100] loss: 0.10020 |  Acc: 29.336% (3755/12800)
[77,   200] loss: 0.10035 |  Acc: 29.906% (3828/12800)
[77,   300] loss: 0.09973 |  Acc: 29.805% (3815/12800)

Epoch: 78
[78,   100] loss: 0.10017 |  Acc: 29.156% (3732/12800)
[78,   200] loss: 0.09993 |  Acc: 30.695% (3929/12800)
[78,   300] loss: 0.09962 |  Acc: 30.344% (3884/12800)

Epoch: 79
[79,   100] loss: 0.09938 |  Acc: 30.016% (3842/12800)
[79,   200] loss: 0.09944 |  Acc: 30.141% (3858/12800)
[79,   300] loss: 0.09938 |  Acc: 30.766% (3938/12800)

Epoch: 80
[80,   100] loss: 0.09926 |  Acc: 30.383% (3889/12800)
[80,   200] loss: 0.09923 |  Acc: 30.688% (3928/12800)
[80,   300] loss: 0.09887 |  Acc: 30.617% (3919/12800)

Epoch: 81
[81,   100] loss: 0.09905 |  Acc: 30.539% (3909/12800)
[81,   200] loss: 0.09867 |  Acc: 30.953% (3962/12800)
[81,   300] loss: 0.09914 |  Acc: 30.492% (3903/12800)

Epoch: 82
[82,   100] loss: 0.09888 |  Acc: 30.406% (3892/12800)
[82,   200] loss: 0.09811 |  Acc: 31.031% (3972/12800)
[82,   300] loss: 0.09870 |  Acc: 30.367% (3887/12800)

Epoch: 83
[83,   100] loss: 0.09833 |  Acc: 31.078% (3978/12800)
[83,   200] loss: 0.09810 |  Acc: 31.375% (4016/12800)
[83,   300] loss: 0.09830 |  Acc: 30.984% (3966/12800)

Epoch: 84
[84,   100] loss: 0.09795 |  Acc: 31.211% (3995/12800)
[84,   200] loss: 0.09800 |  Acc: 31.234% (3998/12800)
[84,   300] loss: 0.09783 |  Acc: 31.195% (3993/12800)

Epoch: 85
[85,   100] loss: 0.09772 |  Acc: 30.898% (3955/12800)
[85,   200] loss: 0.09759 |  Acc: 31.188% (3992/12800)
[85,   300] loss: 0.09736 |  Acc: 31.484% (4030/12800)

Epoch: 86
[86,   100] loss: 0.09705 |  Acc: 31.406% (4020/12800)
[86,   200] loss: 0.09691 |  Acc: 31.906% (4084/12800)
[86,   300] loss: 0.09768 |  Acc: 31.164% (3989/12800)

Epoch: 87
[87,   100] loss: 0.09668 |  Acc: 32.164% (4117/12800)
[87,   200] loss: 0.09678 |  Acc: 31.828% (4074/12800)
[87,   300] loss: 0.09724 |  Acc: 32.000% (4096/12800)

Epoch: 88
[88,   100] loss: 0.09662 |  Acc: 32.141% (4114/12800)
[88,   200] loss: 0.09696 |  Acc: 31.930% (4087/12800)
[88,   300] loss: 0.09622 |  Acc: 31.852% (4077/12800)

Epoch: 89
[89,   100] loss: 0.09669 |  Acc: 31.484% (4030/12800)
[89,   200] loss: 0.09594 |  Acc: 32.875% (4208/12800)
[89,   300] loss: 0.09647 |  Acc: 31.703% (4058/12800)

Epoch: 90
[90,   100] loss: 0.09624 |  Acc: 32.227% (4125/12800)
[90,   200] loss: 0.09622 |  Acc: 32.070% (4105/12800)
[90,   300] loss: 0.09551 |  Acc: 32.773% (4195/12800)

Epoch: 91
[91,   100] loss: 0.09561 |  Acc: 32.508% (4161/12800)
[91,   200] loss: 0.09557 |  Acc: 32.516% (4162/12800)
[91,   300] loss: 0.09585 |  Acc: 32.695% (4185/12800)

Epoch: 92
[92,   100] loss: 0.09530 |  Acc: 32.430% (4151/12800)
[92,   200] loss: 0.09524 |  Acc: 33.086% (4235/12800)
[92,   300] loss: 0.09492 |  Acc: 32.570% (4169/12800)

Epoch: 93
[93,   100] loss: 0.09503 |  Acc: 32.672% (4182/12800)
[93,   200] loss: 0.09509 |  Acc: 32.531% (4164/12800)
[93,   300] loss: 0.09510 |  Acc: 32.938% (4216/12800)

Epoch: 94
[94,   100] loss: 0.09473 |  Acc: 32.641% (4178/12800)
[94,   200] loss: 0.09486 |  Acc: 33.000% (4224/12800)
[94,   300] loss: 0.09411 |  Acc: 33.570% (4297/12800)

Epoch: 95
[95,   100] loss: 0.09465 |  Acc: 33.125% (4240/12800)
[95,   200] loss: 0.09422 |  Acc: 33.344% (4268/12800)
[95,   300] loss: 0.09378 |  Acc: 33.336% (4267/12800)

Epoch: 96
[96,   100] loss: 0.09354 |  Acc: 34.070% (4361/12800)
[96,   200] loss: 0.09407 |  Acc: 33.352% (4269/12800)
[96,   300] loss: 0.09411 |  Acc: 33.227% (4253/12800)

Epoch: 97
[97,   100] loss: 0.09386 |  Acc: 33.922% (4342/12800)
[97,   200] loss: 0.09296 |  Acc: 34.414% (4405/12800)
[97,   300] loss: 0.09320 |  Acc: 33.688% (4312/12800)

Epoch: 98
[98,   100] loss: 0.09326 |  Acc: 33.867% (4335/12800)
[98,   200] loss: 0.09311 |  Acc: 34.383% (4401/12800)
[98,   300] loss: 0.09335 |  Acc: 33.625% (4304/12800)

Epoch: 99
[99,   100] loss: 0.09289 |  Acc: 34.148% (4371/12800)
[99,   200] loss: 0.09284 |  Acc: 34.570% (4425/12800)
[99,   300] loss: 0.09263 |  Acc: 34.562% (4424/12800)

Epoch: 100
[100,   100] loss: 0.09251 |  Acc: 34.922% (4470/12800)
[100,   200] loss: 0.09276 |  Acc: 33.789% (4325/12800)
[100,   300] loss: 0.09267 |  Acc: 34.039% (4357/12800)
Accuracy of the network on the 10000 test images: 35 %
Accuracy of plane : 46 %
Accuracy of   car : 39 %
Accuracy of  bird : 20 %
Accuracy of   cat : 21 %
Accuracy of  deer : 16 %
Accuracy of   dog : 27 %
Accuracy of  frog : 51 %
Accuracy of horse : 42 %
Accuracy of  ship : 45 %
Accuracy of truck : 44 %